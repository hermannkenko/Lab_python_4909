{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f26ddd3",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 9: Build a Log Aggregator\n",
    "\n",
    "In this lab, you will create your own log generator, build a command-line utility that scans log files, summarizes their contents, and provides insight into system behavior. Data structures to track log message levels such as `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.\n",
    "\n",
    "This lab reinforces:\n",
    "- File I/O\n",
    "- Pattern recognition (regex)\n",
    "- Dictionaries and counters\n",
    "- Functions and modularity\n",
    "- CLI arguments, logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ee8a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Create Log files (20%)\n",
    "Using the the following example log format below create a **python file** that will log errors In a structured tree format \n",
    "\n",
    "You will find examples in the folder called Logs that you can use to build your program.\n",
    "\n",
    "Remember set of logs should have a varied levels of log entries (`INFO`, `WARNING`, `ERROR`, `CRITICAL`) and tailored message types for different service components.\n",
    "You must create 5 structured logs here are some examples:\n",
    "\n",
    "    sqldb\n",
    "    ui\n",
    "    frontend.js\n",
    "    backend.js\n",
    "    frontend.flask\n",
    "    backend.flask\n",
    "\n",
    "You may use chat GPT to create sample outputs NOT THE LOGS. IE:\n",
    "\n",
    "    System failure\n",
    "    Database corruption\n",
    "    Disk failure detected\n",
    "    Database corruption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9ba30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log files successfully created in 'Logs' folder!\n"
     ]
    }
   ],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a folder for logs if not exists\n",
    "os.makedirs(\"Logs\", exist_ok=True)\n",
    "\n",
    "# Components to simulate logs for\n",
    "components = [\"sqldb\", \"ui\", \"frontend.js\", \"backend.js\", \"frontend.flask\", \"backend.flask\"]\n",
    "\n",
    "# Possible messages\n",
    "messages = [\n",
    "    \"System failure\", \"Database corruption\", \"Disk failure detected\",\n",
    "    \"Unhandled exception\", \"Service timeout\", \"User login success\",\n",
    "    \"Configuration loaded\", \"Cache miss\", \"API response delayed\",\n",
    "    \"Memory leak detected\"\n",
    "]\n",
    "\n",
    "# Log levels\n",
    "levels = [logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL]\n",
    "\n",
    "# Create separate loggers for each component\n",
    "for component in components:\n",
    "    logger = logging.getLogger(component)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    handler = logging.FileHandler(f\"Logs/{component}.log\")\n",
    "    formatter = logging.Formatter(\"%(asctime)s | %(name)s | %(levelname)s | %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    # Simulate writing 50 log entries per component\n",
    "    current_time = datetime.now()\n",
    "    for _ in range(50):\n",
    "        # Randomize message\n",
    "        message = random.choice(messages)\n",
    "        level = random.choice(levels)\n",
    "\n",
    "        # Log it\n",
    "        logger.log(level, message)\n",
    "\n",
    "        # Artificially advance time a bit for realism\n",
    "        current_time += timedelta(seconds=random.randint(1, 120))\n",
    "\n",
    "    logger.handlers.clear()  # Clean up handlers so it doesn't duplicate logs if rerun\n",
    "\n",
    "print(\"Log files successfully created in 'Logs' folder!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5255ab",
   "metadata": {},
   "source": [
    "\n",
    "### Example Log Format\n",
    "\n",
    "You will work with logs that follow this simplified structure:\n",
    "\n",
    "```\n",
    "2025-04-11 23:20:36,913 | my_app | INFO | Request completed\n",
    "2025-04-11 23:20:36,914 | my_app.utils | ERROR | Unhandled exception\n",
    "2025-04-11 23:20:36,914 | my_app.utils.db | CRITICAL | Disk failure detected\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659dfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5f6e84",
   "metadata": {},
   "source": [
    "## Part 2: Logging the Log File (40%)\n",
    "    New File\n",
    "### Part 2a: Read the Log File (see lab 7) (10%)\n",
    "\n",
    "\n",
    "Write a function to read the contents of a log file into a list of lines. Handle file errors gracefully.\n",
    "\n",
    "### Part 2b: Parse Log Lines (see code below if you get stuck) (10%)\n",
    "\n",
    "Use a regular expression to extract:\n",
    "- Timestamp\n",
    "- Log name\n",
    "- Log level\n",
    "- Message\n",
    "\n",
    "### Part 2c: Count Log Levels (20%)\n",
    "\n",
    "Create a function to count how many times each log level appears. Store the results in a dictionary. Then output it as a Json File\n",
    "You may pick your own format but here is an example. \n",
    "```python\n",
    "{\n",
    "    \"INFO\": \n",
    "    {\n",
    "        \"Request completed\": 42, \n",
    "        \"Heartbeat OK\": 7\n",
    "    }\n",
    "\n",
    "    \"WARNING\":\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f8a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission\n",
    "\n",
    "def read_log_file(file_path):\n",
    "    \"\"\"Reads a log file and returns a list of lines.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5353ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2214599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file Logs/your_log_file.log was not found.\n",
      "Log summary saved to 'log_summary.json'!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_log_levels(log_lines):\n",
    "    \"\"\"\n",
    "    Counts how many times each message appears for each log level.\n",
    "    \"\"\"\n",
    "    log_summary = {}\n",
    "\n",
    "    for line in log_lines:\n",
    "        timestamp, logger_name, log_level, message = parse_log_line(line)\n",
    "        if log_level and message:\n",
    "            if log_level not in log_summary:\n",
    "                log_summary[log_level] = {}\n",
    "            if message not in log_summary[log_level]:\n",
    "                log_summary[log_level][message] = 0\n",
    "            log_summary[log_level][message] += 1\n",
    "\n",
    "    return log_summary\n",
    "\n",
    "def save_summary_to_json(summary, output_file):\n",
    "    \"\"\"Saves the log summary dictionary into a JSON file.\"\"\"\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(summary, file, indent=4)\n",
    "# Set the path to the log file\n",
    "log_file_path = \"Logs/your_log_file.log\"  # Update the filename if needed\n",
    "\n",
    "# 2a: Read the file\n",
    "log_lines = read_log_file(log_file_path)\n",
    "\n",
    "# 2b + 2c: Parse lines and count\n",
    "summary = count_log_levels(log_lines)\n",
    "\n",
    "# Save to JSON\n",
    "save_summary_to_json(summary, \"log_summary.json\")\n",
    "\n",
    "print(\"Log summary saved to 'log_summary.json'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045c30f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Generate Summary Report (40%)\n",
    "    New File\n",
    "### Step 3a (20%):\n",
    " Develop a function that continuously monitors your JSON file(s) and will print a real-time summary of log activity. It should keep count of the messages grouped by log level (INFO, WARNING, ERROR, CRITICAL) and display only the critical messages. (I.e. If new data comes in the summary will change and a new critical message will be printed)\n",
    " - note: do not reprocess the entire file on each update.  \n",
    "\n",
    "### Step 3a: Use a Matplotlib (Lecture 10) (20%)\n",
    "Develop a function that continuously monitors your JSON file(s) and will graph in real-time a bar or pie plot of each of the errors.  (a graph for each log level). \n",
    "- The graph should show the distribution of log messages by level  (INFO, WARNING, ERROR, CRITICAL)  \n",
    "\n",
    "\n",
    "### Critical notes:\n",
    "- Your code mus use Daemon Threads (Lecture 14)\n",
    "- 3a and 3b do not need to run at the same time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea4429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission\n",
    "\n",
    "import threading\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "def monitor_json_file(file_path):\n",
    "    \"\"\"Continuously monitors the JSON file and prints critical messages in real time.\"\"\"\n",
    "    last_size = 0\n",
    "    processed_critical = set()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            current_size = os.path.getsize(file_path)\n",
    "            if current_size != last_size:\n",
    "                last_size = current_size\n",
    "\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "\n",
    "                print(\"\\nUpdated Log Summary:\")\n",
    "                for level, messages in data.items():\n",
    "                    total = sum(messages.values())\n",
    "                    print(f\"{level}: {total} messages\")\n",
    "\n",
    "                    if level == \"CRITICAL\":\n",
    "                        for message, count in messages.items():\n",
    "                            if message not in processed_critical:\n",
    "                                print(f\"New CRITICAL message detected: {message} (Count: {count})\")\n",
    "                                processed_critical.add(message)\n",
    "            time.sleep(2)  # check every 2 seconds\n",
    "        except Exception as e:\n",
    "            print(f\"Error monitoring JSON file: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "def start_monitoring(file_path):\n",
    "    monitor_thread = threading.Thread(target=monitor_json_file, args=(file_path,), daemon=True)\n",
    "    monitor_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b26eb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a sample regex that parses a log file and extracts relevant information. \n",
    "# you will need to modify it. Review Lecture 11\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_log_line(line):\n",
    "    \"\"\"\n",
    "    Parses a log line into timestamp, logger name, log level, and message.\n",
    "    Example format expected: '2023-04-27 10:05:01,123 - myLogger - INFO - Request completed'\n",
    "    \"\"\"\n",
    "    log_pattern = r\"^(.*?)\\s\\|\\s(\\w+)\\s\\|\\s(\\w+)\\s\\|\\s(.*)$\"\n",
    "    match = re.match(log_pattern, line)\n",
    "    if match:\n",
    "        timestamp, logger_name, log_level, message = match.groups()\n",
    "        return timestamp, logger_name, log_level, message\n",
    "    else:\n",
    "        return None, None, None, None  # In case the line is not formatted correctly\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
